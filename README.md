# ğŸŒ Transformer-based English to Spanish Machine Translation

A complete machine translation pipeline using the Transformer architecture from the paper ["Attention is All You Need"](https://arxiv.org/abs/1706.03762), built from scratch in PyTorch.

This project supports:
- ğŸ§  End-to-end training on a open source English to Spanish translations dataset
- ğŸ“Š Evaluation with BLEU and ROUGE using Hugging Face's `evaluate`
- ğŸ’¬ Streamlit demo for real-time translation
- ğŸ” Attention visualization (optional)
- ğŸ” Beam search decoding
- â˜ï¸ Push to Hugging Face Hub for open source sharing (pending)

---

## ğŸš€ Project Structure

```bash
.
â”œâ”€â”€ app.py                    # Streamlit web UI
â”œâ”€â”€ train.py                  # Training loop with evaluation and checkpointing
â”œâ”€â”€ translation_model.py      # Core Transformer architecture & utilities
â”œâ”€â”€ decode_utils.py           # decoding functions BEAM and Greedy
â”œâ”€â”€ english_spanish.csv       # Dataset
â”œâ”€â”€ requirements.txt          # Requirements file 
â””â”€â”€ README.md

---

## ğŸ§  Model Architecture
This is a classic encoder-decoder Transformer, built using PyTorch:

- Multi-Head Self-Attention
- Positional Encoding
- Layer Normalization
- Masked Decoding
- Beam Search & Greedy Decoding

```bash

The model was trained on ~300,000 English-Spanish pairs.

## Results (trained on smaller dataset):
BLEU Scores: ~36%
ROUGE Scores: ~46%

## Next Steps:
âœ… Pretrained multilingual models like MBart50 for faster convergence and real-world performance
âœ… Replacing the custom tokenizer with AutoTokenizer from Hugging Face for subword and BPE support
âœ… Scaling up with larger datasets (e.g., Europarl, Tatoeba), longer training, and better hardware utilization
âœ… Cloud deployment using AWS or GCP to host the model and Streamlit app as a translation service
